{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Resumen NLTK: Acceso a corpus de texto y recursos léxicos\n",
    "\n",
    "Este resumen se corresponde con el capítulo 2 del NLTK Book Accessing Text Corpora and Lexical Resources.\n",
    "Corpus no anotados: el Proyecto Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'austen-emma.txt',\n",
       " u'austen-persuasion.txt',\n",
       " u'austen-sense.txt',\n",
       " u'bible-kjv.txt',\n",
       " u'blake-poems.txt',\n",
       " u'bryant-stories.txt',\n",
       " u'burgess-busterbrown.txt',\n",
       " u'carroll-alice.txt',\n",
       " u'chesterton-ball.txt',\n",
       " u'chesterton-brown.txt',\n",
       " u'chesterton-thursday.txt',\n",
       " u'edgeworth-parents.txt',\n",
       " u'melville-moby_dick.txt',\n",
       " u'milton-paradise.txt',\n",
       " u'shakespeare-caesar.txt',\n",
       " u'shakespeare-hamlet.txt',\n",
       " u'shakespeare-macbeth.txt',\n",
       " u'whitman-leaves.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "emma = gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192427\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(gutenberg.words('austen-emma.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'[', u'Emma', u'by', u'Jane', u'Austen', u'1816', ...]\n"
     ]
    }
   ],
   "source": [
    "# Cargar versión de Alicia descompuesta por palabras\n",
    "emma_frases =gutenberg.words('austen-emma.txt')\n",
    "print(emma_frases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  len(emma_frases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once\n"
     ]
    }
   ],
   "source": [
    "cruda = gutenberg.raw('carroll-alice.txt')\n",
    "print(cruda[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'[', u'Alice', u\"'\", u's', u'Adventures', u'in', ...]\n"
     ]
    }
   ],
   "source": [
    "alicia = gutenberg.words('carroll-alice.txt')\n",
    "print(alicia[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"[Alice's\", u'Adventures', u'in', u'Wonderland', u'by', u'Lewis', u'Carroll', u'1865]', u'CHAPTER', u'I.', u'Down', u'the', u'Rabbit-Hole', u'Alice', u'was', u'beginning', u'to', u'get', u'very', u'tired', u'of', u'sitting', u'by', u'her', u'sister', u'on', u'the', u'bank,', u'and', u'of', u'having', u'nothing', u'to', u'do:', u'once', u'or', u'twice', u'she', u'had', u'peeped', u'into', u'the', u'book', u'her', u'sister', u'was', u'reading,', u'but', u'it', u'had', u'no', u'pictures', u'or', u'conversations', u'in', u'it,', u\"'and\", u'what', u'is', u'the', u'use', u'of', u'a', u\"book,'\", u'thought', u'Alice', u\"'without\", u'pictures', u'or', u\"conversation?'\", u'So', u'she', u'was', u'considering', u'in', u'her', u'own', u'mind', u'(as', u'well', u'as', u'she', u'could,', u'for', u'the', u'hot', u'day', u'made', u'her', u'feel', u'very', u'sleepy', u'and', u'stupid),', u'whether', u'the', u'pleasure', u'of', u'making', u'a', u'daisy-chain', u'would', u'be', u'worth', u'the', u'trouble', u'of', u'getting', u'up', u'and', u'picking', u'the', u'daisies,', u'when', u'suddenly', u'a', u'White', u'Rabbit', u'with', u'pink', u'eyes', u'ran', u'close', u'by', u'her.', u'There', u'was', u'nothing', u'so', u'VERY', u'remarkable', u'in', u'that;', u'nor', u'did', u'Alice', u'think', u'it', u'so', u'VERY', u'much', u'out', u'of', u'the', u'way', u'to', u'hear', u'the', u'Rabbit', u'say', u'to', u'itself,', u\"'Oh\", u'dear!', u'Oh', u'dear!', u'I', u'shall', u'be', u\"late!'\", u'(when', u'she', u'thought', u'it', u'over', u'afterwards,', u'it', u'occurred', u'to', u'her', u'that', u'she', u'ought', u'to', u'have', u'wondered', u'at', u'this,', u'but', u'at', u'the', u'time', u'it', u'all', u'seemed', u'quite', u'natural);', u'but', u'when', u'the', u'Rabbit', u'actually', u'TOOK', u'A', u'WATCH', u'OUT', u'OF', u'ITS', u'WAISTCOAT-POCKET,', u'and']\n"
     ]
    }
   ],
   "source": [
    "alicia_tokens = cruda.split()\n",
    "print(alicia_tokens[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('alicia tiene', 1703, 'oraciones')\n",
      "[[u'CHAPTER', u'I', u'.'], [u'Down', u'the', u'Rabbit', u'-', u'Hole'], [u'Alice', u'was', u'beginning', u'to', u'get', u'very', u'tired', u'of', u'sitting', u'by', u'her', u'sister', u'on', u'the', u'bank', u',', u'and', u'of', u'having', u'nothing', u'to', u'do', u':', u'once', u'or', u'twice', u'she', u'had', u'peeped', u'into', u'the', u'book', u'her', u'sister', u'was', u'reading', u',', u'but', u'it', u'had', u'no', u'pictures', u'or', u'conversations', u'in', u'it', u',', u\"'\", u'and', u'what', u'is', u'the', u'use', u'of', u'a', u'book', u\",'\", u'thought', u'Alice', u\"'\", u'without', u'pictures', u'or', u'conversation', u\"?'\"], [u'So', u'she', u'was', u'considering', u'in', u'her', u'own', u'mind', u'(', u'as', u'well', u'as', u'she', u'could', u',', u'for', u'the', u'hot', u'day', u'made', u'her', u'feel', u'very', u'sleepy', u'and', u'stupid', u'),', u'whether', u'the', u'pleasure', u'of', u'making', u'a', u'daisy', u'-', u'chain', u'would', u'be', u'worth', u'the', u'trouble', u'of', u'getting', u'up', u'and', u'picking', u'the', u'daisies', u',', u'when', u'suddenly', u'a', u'White', u'Rabbit', u'with', u'pink', u'eyes', u'ran', u'close', u'by', u'her', u'.'], [u'There', u'was', u'nothing', u'so', u'VERY', u'remarkable', u'in', u'that', u';', u'nor', u'did', u'Alice', u'think', u'it', u'so', u'VERY', u'much', u'out', u'of', u'the', u'way', u'to', u'hear', u'the', u'Rabbit', u'say', u'to', u'itself', u',', u\"'\", u'Oh', u'dear', u'!'], [u'Oh', u'dear', u'!'], [u'I', u'shall', u'be', u'late', u\"!'\"], [u'(', u'when', u'she', u'thought', u'it', u'over', u'afterwards', u',', u'it', u'occurred', u'to', u'her', u'that', u'she', u'ought', u'to', u'have', u'wondered', u'at', u'this', u',', u'but', u'at', u'the', u'time', u'it', u'all', u'seemed', u'quite', u'natural', u');', u'but', u'when', u'the', u'Rabbit', u'actually', u'TOOK', u'A', u'WATCH', u'OUT', u'OF', u'ITS', u'WAISTCOAT', u'-', u'POCKET', u',', u'and', u'looked', u'at', u'it', u',', u'and', u'then', u'hurried', u'on', u',', u'Alice', u'started', u'to', u'her', u'feet', u',', u'for', u'it', u'flashed', u'across', u'her', u'mind', u'that', u'she', u'had', u'never', u'before', u'seen', u'a', u'rabbit', u'with', u'either', u'a', u'waistcoat', u'-', u'pocket', u',', u'or', u'a', u'watch', u'to', u'take', u'out', u'of', u'it', u',', u'and', u'burning', u'with', u'curiosity', u',', u'she', u'ran', u'across', u'the', u'field', u'after', u'it', u',', u'and', u'fortunately', u'was', u'just', u'in', u'time', u'to', u'see', u'it', u'pop', u'down', u'a', u'large', u'rabbit', u'-', u'hole', u'under', u'the', u'hedge', u'.'], [u'In', u'another', u'moment', u'down', u'went', u'Alice', u'after', u'it', u',', u'never', u'once', u'considering', u'how', u'in', u'the', u'world', u'she', u'was', u'to', u'get', u'out', u'again', u'.']]\n"
     ]
    }
   ],
   "source": [
    "# Alicia segmentada por oraciones\n",
    "alicia_sent = gutenberg.sents('carroll-alice.txt')\n",
    "print('alicia tiene',len(alicia_sent), 'oraciones')\n",
    "print(alicia_sent[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[u'[', u'Alice', u\"'\", u's', u'Adventures', u'in', u'Wonderland', u'by', u'Lewis', u'Carroll', u'1865', u']']], [[u'CHAPTER', u'I', u'.'], [u'Down', u'the', u'Rabbit', u'-', u'Hole']], ...]\n",
      "('\\n', '---------------------------------------------------------------------------', '\\n')\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los cinco primeros párrafos\n",
    "alicia_para = gutenberg.paras('carroll-alice.txt')\n",
    "for i in alicia_para[3]:\n",
    "    print(alicia_para)\n",
    "    print('\\n','-'*75,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "817\n",
      "1703\n"
     ]
    }
   ],
   "source": [
    "# Averiguamos el número de pàrrafos, oraciones y palabras\n",
    "print(len(alicia_para))\n",
    "print(len(alicia_sent))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "# Estadísticas Gutenberg\n",
    "# Promedio palabras por oración\n",
    "co = 0\n",
    "for libro in gutenberg.fileids():\n",
    "     co = co + len(gutenberg.words(libro))/len(gutenberg.sents(libro))\n",
    "Total_palabras = co / len(gutenberg.fileids())\n",
    "print(Total_palabras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Promedio caracteres\n",
    "co = 0\n",
    "for libro in gutenberg.fileids():\n",
    "    co = co + len(gutenberg.raw(libro))/len(gutenberg.words(libro))\n",
    "Total_caract = co / len(gutenberg.fileids())\n",
    "print(Total_caract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Promedio oraciones por párrafo\n",
    "co = 0\n",
    "for libro in gutenberg.fileids():\n",
    "     co = co + len(gutenberg.sents(libro))/len(gutenberg.paras(libro))\n",
    "Total_oracion = co / len(gutenberg.fileids())\n",
    "print(Total_oracion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt\n",
      "4.0\n",
      "24.0\n",
      "3.0\n",
      "austen-persuasion.txt\n",
      "4.0\n",
      "26.0\n",
      "3.0\n",
      "austen-sense.txt\n",
      "4.0\n",
      "28.0\n",
      "2.0\n",
      "bible-kjv.txt\n",
      "4.0\n",
      "33.0\n",
      "1.0\n",
      "blake-poems.txt\n",
      "4.0\n",
      "19.0\n",
      "1.0\n",
      "bryant-stories.txt\n",
      "4.0\n",
      "19.0\n",
      "2.0\n",
      "burgess-busterbrown.txt\n",
      "4.0\n",
      "17.0\n",
      "3.0\n",
      "carroll-alice.txt\n",
      "4.0\n",
      "20.0\n",
      "2.0\n",
      "chesterton-ball.txt\n",
      "4.0\n",
      "20.0\n",
      "2.0\n",
      "chesterton-brown.txt\n",
      "4.0\n",
      "22.0\n",
      "3.0\n",
      "chesterton-thursday.txt\n",
      "4.0\n",
      "18.0\n",
      "2.0\n",
      "edgeworth-parents.txt\n",
      "4.0\n",
      "20.0\n",
      "2.0\n",
      "melville-moby_dick.txt\n",
      "4.0\n",
      "25.0\n",
      "3.0\n",
      "milton-paradise.txt\n",
      "4.0\n",
      "52.0\n",
      "63.0\n",
      "shakespeare-caesar.txt\n",
      "4.0\n",
      "11.0\n",
      "2.0\n",
      "shakespeare-hamlet.txt\n",
      "4.0\n",
      "12.0\n",
      "3.0\n",
      "shakespeare-macbeth.txt\n",
      "4.0\n",
      "12.0\n",
      "2.0\n",
      "whitman-leaves.txt\n",
      "4.0\n",
      "36.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Solución\n",
    "oracion = len (gutenberg.sents(libro))\n",
    "parrafo = len (gutenberg.paras(libro))\n",
    "for libro in gutenberg.fileids():\n",
    "    print(libro)\n",
    "    print(float(len(gutenberg.raw(libro))/len(gutenberg.words(libro))))\n",
    "    print(float(len(gutenberg.words(libro))/len(gutenberg.sents(libro))))\n",
    "    print(float(len(gutenberg.sents(libro))/len(gutenberg.paras(libro))))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Utilizamos el corpus cess_esp para Español\n",
    "from nltk.corpus import cess_esp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'El', u'grupo', u'estatal', u'Electricit\\xe9_de_France', u'-Fpa-', u'EDF', u'-Fpt-', u'anunci\\xf3', u'hoy', u',', u'jueves', u',', u'la', u'compra', u'del']\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el primer documento (desconocido) para probar.\n",
    "palabras = cess_esp.words(cess_esp.fileids()[0])\n",
    "print(palabras[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'El', u'grupo', u'estatal', u'Electricit\\xe9_de_France', u'-Fpa-', u'EDF', u'-Fpt-', u'anunci\\xf3', u'hoy', u',', u'jueves', u',', u'la', u'compra', u'del', u'51_por_ciento', u'de', u'la', u'empresa', u'mexicana', u'Electricidad_\\xc1guila_de_Altamira', u'-Fpa-', u'EAA', u'-Fpt-', u',', u'creada', u'por', u'el', u'japon\\xe9s', u'Mitsubishi_Corporation', u'para', u'poner_en_marcha', u'una', u'central', u'de', u'gas', u'de', u'495', u'megavatios', u'.'], [u'Una', u'portavoz', u'de', u'EDF', u'explic\\xf3', u'a', u'EFE', u'que', u'el', u'proyecto', u'para', u'la', u'construcci\\xf3n', u'de', u'Altamira_2', u',', u'al', u'norte', u'de', u'Tampico', u',', u'prev\\xe9', u'la', u'utilizaci\\xf3n', u'de', u'gas', u'natural', u'como', u'combustible', u'principal', u'en', u'una', u'central', u'de', u'ciclo', u'combinado', u'que', u'debe', u'empezar', u'a', u'funcionar', u'en', u'mayo_del_2002', u'.'], [u'La', u'electricidad', u'producida', u'pasar\\xe1', u'a', u'la', u'red', u'el\\xe9ctrica', u'p\\xfablica', u'de', u'M\\xe9xico', u'en_virtud_de', u'un', u'acuerdo', u'de', u'venta', u'de', u'energ\\xeda', u'de', u'EAA', u'con', u'la', u'Comisi\\xf3n_Federal_de_Electricidad', u'-Fpa-', u'CFE', u'-Fpt-', u'por', u'una', u'duraci\\xf3n', u'de', u'25', u'a\\xf1os', u'.'], [u'EDF', u',', u'que', u'no', u'quiso', u'revelar', u'cu\\xe1nto', u'*0*', u'pag\\xf3', u'por', u'su', u'participaci\\xf3n', u'mayoritaria', u'en', u'EAA', u',', u'intervendr\\xe1', u'como', u'asistente', u'en', u'la', u'construcci\\xf3n', u'de', u'Altamira_2', u'y', u',', u'posteriormente', u',', u'*0*', u'se', u'encargar\\xe1', u'de', u'explotarla', u'como', u'principal', u'accionista', u'.'], [u'EDF', u'y', u'Mitsubishi', u'participaron', u'en', u'1998', u'en', u'la', u'licitaci\\xf3n', u'de', u'licencias', u'para', u'construir', u'centrales', u'el\\xe9ctricas', u'en', u'M\\xe9xico', u'y', u'*0*', u'se', u'quedaron', u'con', u'dos', u'cada', u'una', u':', u'R\\xedo_Bravo', u'y', u'Saltillo', u'para', u'la', u'compa\\xf1\\xeda', u'francesa', u'y', u'Altamira', u'y', u'Tuxp\\xe1n', u'para', u'la', u'japonesa', u'.']]\n"
     ]
    }
   ],
   "source": [
    "oraciones = cess_esp.sents(cess_esp.fileids()[0])\n",
    "print(oraciones[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "609"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cess_esp.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "64\n",
      "70\n",
      "63\n",
      "69\n",
      "72\n",
      "65\n",
      "74\n",
      "65\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la longitud promedio de palabras y el número de palabras  promedio por oración para los diez primeros documentos\n",
    "\n",
    "for doc  in cess_esp.fileids()[:10]:\n",
    "    print(len(cess_esp.raw(doc))/len(cess_esp.words(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10017_2000\n",
      "('Media palabras:', 42)\n",
      "10044_2000\n",
      "('Media palabras:', 47)\n",
      "10049_2000\n",
      "('Media palabras:', 31)\n",
      "10055_2000\n",
      "('Media palabras:', 30)\n",
      "10080_2000\n",
      "('Media palabras:', 34)\n",
      "10084_2000\n",
      "('Media palabras:', 42)\n",
      "10084_2000\n",
      "('Media palabras:', 24)\n",
      "10127_2000\n",
      "('Media palabras:', 40)\n",
      "10127_2000\n",
      "('Media palabras:', 32)\n",
      "10127_2000\n",
      "('Media palabras:', 35)\n"
     ]
    }
   ],
   "source": [
    "for doc  in cess_esp.fileids()[:10]:\n",
    "    print(doc[0:10])\n",
    "    print('Media palabras:',len(cess_esp.words(doc))/len(cess_esp.sents(doc)))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus categorizados y anotados: el Corpus de Brown\n",
    "\n",
    "El Corpus de Brown fue el primer gran corpus orientado a tareas de PLN. Desarrollado en la Universidad de Brown, contiene más de un millón de palabras provenientes de 500 fuentes. La principal catacterística de este corpus es que sus textos están categorizados por género."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'ca01', u'ca02', u'ca03', u'ca04', u'ca05']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.fileids()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las principales diferencias con otros corpus vistos anteriormente es que el de Brown está categorizado: los textos están agrupados según su género o temática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'adventure',\n",
       " u'belles_lettres',\n",
       " u'editorial',\n",
       " u'fiction',\n",
       " u'government',\n",
       " u'hobbies',\n",
       " u'humor',\n",
       " u'learned',\n",
       " u'lore',\n",
       " u'mystery',\n",
       " u'news',\n",
       " u'religion',\n",
       " u'reviews',\n",
       " u'romance',\n",
       " u'science_fiction']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'The', u'Fulton', u'County', u'Grand', u'Jury']\n",
      "[[u'Now', u'that', u'he', u'knew', u'himself', u'to', u'be', u'self', u'he', u'was', u'free', u'to', u'grok', u'ever', u'closer', u'to', u'his', u'brothers', u',', u'merge', u'without', u'let', u'.'], [u\"Self's\", u'integrity', u'was', u'and', u'is', u'and', u'ever', u'had', u'been', u'.'], [u'Mike', u'stopped', u'to', u'cherish', u'all', u'his', u'brother', u'selves', u',', u'the', u'many', u'threes-fulfilled', u'on', u'Mars', u',', u'corporate', u'and', u'discorporate', u',', u'the', u'precious', u'few', u'on', u'Earth', u'--', u'the', u'unknown', u'powers', u'of', u'three', u'on', u'Earth', u'that', u'would', u'be', u'his', u'to', u'merge', u'with', u'and', u'cherish', u'now', u'that', u'at', u'last', u'long', u'waiting', u'he', u'grokked', u'and', u'cherished', u'himself', u'.'], [u'Mike', u'remained', u'in', u'trance', u';', u';'], [u'there', u'was', u'much', u'to', u'grok', u',', u'loose', u'ends', u'to', u'puzzle', u'over', u'and', u'fit', u'into', u'his', u'growing', u'--', u'all', u'that', u'he', u'had', u'seen', u'and', u'heard', u'and', u'been', u'at', u'the', u'Archangel', u'Foster', u'Tabernacle', u'(', u'not', u'just', u'cusp', u'when', u'he', u'and', u'Digby', u'had', u'come', u'face', u'to', u'face', u'alone', u')', u'why', u'Bishop', u'Senator', u'Boone', u'made', u'him', u'warily', u'uneasy', u',', u'how', u'Miss', u'Dawn', u'Ardent', u'tasted', u'like', u'a', u'water', u'brother', u'when', u'she', u'was', u'not', u',', u'the', u'smell', u'of', u'goodness', u'he', u'had', u'incompletely', u'grokked', u'in', u'the', u'jumping', u'up', u'and', u'down', u'and', u'wailing', u'--']]\n"
     ]
    }
   ],
   "source": [
    "noticia = brown.words(categories='news')\n",
    "print(noticia[:5])\n",
    "sci = brown.sents(categories='science_fiction')\n",
    "print(sci[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a calcular la frecuencia de distribución de distintos verbos modales para cada categoría. Para ello vamos a calcular una distribución de frecuencia condicional que calcule la frecuencia de cada palabra para cada categoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import ConditionalFreqDist\n",
    "modals = 'can could would should must may might'.split()\n",
    "modals_cfd = ConditionalFreqDist(\n",
    "                          (category, word) \n",
    "                          for category in brown.categories() \n",
    "                          for word in brown.words(categories=category)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   can  could  would should   must    may  might \n",
      "      adventure     46    151    191     15     27      5     58 \n",
      " belles_lettres    246    213    392    102    170    207    113 \n",
      "      editorial    121     56    180     88     53     74     39 \n",
      "        fiction     37    166    287     35     55      8     44 \n",
      "     government    117     38    120    112    102    153     13 \n",
      "        hobbies    268     58     78     73     83    131     22 \n",
      "          humor     16     30     56      7      9      8      8 \n",
      "        learned    365    159    319    171    202    324    128 \n",
      "           lore    170    141    186     76     96    165     49 \n",
      "        mystery     42    141    186     29     30     13     57 \n",
      "           news     93     86    244     59     50     66     38 \n",
      "       religion     82     59     68     45     54     78     12 \n",
      "        reviews     45     40     47     18     19     45     26 \n",
      "        romance     74    193    244     32     45     11     51 \n",
      "science_fiction     16     49     79      3      8      4     12 \n",
      "('\\n', '---------------------------------------------------------------------------', '\\n')\n",
      "           can should  would \n",
      "fiction     37     35    287 \n"
     ]
    }
   ],
   "source": [
    "modals_cfd.tabulate(conditions=brown.categories(), samples=modals)\n",
    "\n",
    "print('\\n', '-' * 75, '\\n')\n",
    "\n",
    "# imprimo solo algunos verbos modales para la categoría fiction\n",
    "modals_cfd.tabulate(conditions=['fiction'], samples=['can', 'should', 'would'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Información morfológica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'The', u'AT'),\n",
       " (u'Fulton', u'NP-TL'),\n",
       " (u'County', u'NN-TL'),\n",
       " (u'Grand', u'JJ-TL'),\n",
       " (u'Jury', u'NN-TL'),\n",
       " (u'said', u'VBD'),\n",
       " (u'Friday', u'NR'),\n",
       " (u'an', u'AT'),\n",
       " (u'investigation', u'NN'),\n",
       " (u'of', u'IN')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_tag = brown.tagged_words(categories='news')\n",
    "b_tag[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear una nueva frecuencia de distribución condicional para calcular la frecuencia de aparición de las etiquetas, teniendo en cuenta la categoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morfo = ConditionalFreqDist((category, item[1]) \n",
    "                          for category in brown.categories() \n",
    "                          for item in brown.tagged_words(categories=category)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   NN    VB   VBD   VBN    JJ    IN    AT \n",
      "      adventure  8051  2170  3702  1276  2687  5908  5531 \n",
      " belles_lettres 21800  4829  3501  4223 10414 19083 14898 \n",
      "      editorial  7675  2129   700  1491  3593  6204  5311 \n",
      "        fiction  7815  2173  3027  1497  2958  6012  5439 \n",
      "     government  9877  1833   405  2190  4173  8596  5716 \n",
      "        hobbies 12465  2966   617  2252  4883  8591  6946 \n",
      "          humor  2567   656   699   478  1078  1926  1655 \n",
      "        learned 29194  4342  1481  6044 12294 21757 16828 \n",
      "           lore 14707  3083  2272  2822  6475 12074  9936 \n",
      "        mystery  6461  2026  2645  1161  2109  4692  4321 \n",
      "           news 13162  2440  2524  2269  4392 10616  8893 \n",
      "       religion  4923  1275   511   931  2327  4266  3327 \n",
      "        reviews  5066   872   504   875  2742  4040  3447 \n",
      "        romance  7166  2404  3048  1359  3180  5616  4671 \n",
      "science_fiction  1541   495   531   318   723  1176  1040 \n"
     ]
    }
   ],
   "source": [
    "morfo.tabulate(conditions=brown.categories(), \n",
    "                  samples='NN VB VBD VBN JJ IN AT'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos Léxicos: WordNet\n",
    "Es similar a un diccionario pero está organizado por synsets (conjunto de palabras sinónimas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('sword.n.01')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('sword')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la palabra sword solo aparece en un synset, lo que implica que solo tiene un sentido. Además, sabemos que es un sustantivo, porque el nombre de synset está etiquetado como n.\n",
    "\n",
    "Por su parte, la palabra car es polisémica y aparece en cinco sentidos, toso ellos sustantivos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'sword', u'blade', u'brand', u'steel']\n",
      "a cutting or thrusting weapon that has a long metal blade and a hilt with a hand guard\n",
      "([u'cable_car', u'car'], u'a conveyance for passengers or freight on a cable railway')\n",
      "([u'cable_car', u'car'], u'a conveyance for passengers or freight on a cable railway')\n",
      "[u'they took a cable car to the top of the mountain']\n"
     ]
    }
   ],
   "source": [
    "sword = wn.synsets('sword')[0]\n",
    "print(sword.lemma_names()) # imprime los lemas del synset => sinónimos\n",
    "print(sword.definition()) # imprime la definición del synset\n",
    "\n",
    "# hacemos lo mismo con car\n",
    "car = wn.synsets('car')\n",
    "cable_car = car[-1]\n",
    "print(cable_car.lemma_names(), cable_car.definition())\n",
    "print(car[-1].lemma_names(), car[-1].definition()) # esta línea es equivalente a la anterior.\n",
    "\n",
    "# imprimo las oraciones de ejemplo\n",
    "print(cable_car.examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
